{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neurais recorrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdução às RNNs\n",
    "\n",
    "As arquiteturas de redes neurais vistas até agora foram treinadas usando apenas entradas atuais. Nelas, não são consideradas entradas anteriores ao gerar a saída atual. Em outras palavras, nossos sistemas não continham nenhum elemento de **memória**. As RNNs lidam com essa questão básica e importante usando **memória** (ou seja, entradas passadas da rede) ao produzir a entrada atual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. História das RNNs\n",
    "\n",
    "Depois da primeira onda de redes neurais artificiais, nos anos 1980, ficou claro que as redes feedforward são limitadas, já que não são capazes de capturar dependências temporais. Modelar dependências temporais é crucial na maioria dos programas do mundo real, já que os sinais naturais, como a fala e o vídeo, têm propriedades de variação de tempo e se caracterizam por ter dependências ao longo do tempo. Além disso, as redes neurais biológicas também têm conexões recorrentes, então aplicar recorrência às redes neurais feedforward artificiais fez todo sentido.\n",
    "\n",
    "A primeira tentativa de adicionar memória às redes neurais ocorreu em 1989, através das redes neurais com atraso de tempo (Time Delay Neural Network, ou TDNN). Nelas, as entradas de timesteps passados são introduzidas na entrada da rede, alterando as verdadeiras entradas externas. Isso trouxe a clara vantagem de permitir que a rede visse além do timestep atual, mas também trouxe uma desvantagem inegável, já que as dependências temporais ficam restritas à janela de tempo escolhida.\n",
    "\n",
    "<img src='../img/tdnn.png' width=350px>\n",
    "\n",
    "As RNNs simples, conhecidas também como redes de Elman e redes de Jordan, surgiram em 1990. Falaremos mais sobre elas ao longo da aula.\n",
    "\n",
    "<img src='../img/rnn_elman.png' width='350px'>\n",
    "\n",
    "Foi constatado, no início dos anos 1990, que todas essas redes sofrem do chamado \"problema da dissipição do gradiente\" (the vanishing gradient problem), onde as contribuições de informação deterioram-se geometricamente ao longo do tempo e, por conta disso, capturar relações que envolvem mais de 8 ou 10 passos anteriores é praticamente impossível. Como você deve lembrar, ao treinarmos a rede usamos **retropropagação**, processo no qual as matrizes de peso são ajustadas usando um **gradiente**. Os gradientes são calculados pela multiplicação sucessiva de derivadas, e o valor dessas derivadas pode ser tão pequeno que essas multiplicações sucessivas fazem com que o gradiente praticamente desapareça. O problema da dissipação do gradiente será discutido mais a fundo em breve. Apesar da elegância das RNNs que existiam até então, todas tinham essa grande falha.\n",
    "\n",
    "Em meados dos anos 1990, as redes de longa memória de curto prazo (Long Short-Term Memory, ou LSTM) foram inventadas para resolver exaatamente este problema. A grande novidade das LSTMs era a ideia de que alguns sinais, que chamamos de variáveis de estado, podem ser mantidos fixos através de portões e reintroduzidos ou não em um momento apropriado no futuro. Assim, intervalos de tempo arbitrários podem ser representados e dependências temporais podem ser capturadas. Todos os detalhes das LSTMs serão abordados na [aula 2](https://github.com/vilacham/rnn_dlnd/blob/master/lesson_2_lstm/lstm_ptbr.ipynb).\n",
    "\n",
    "<img src='../img/lstm.png' width=350px>\n",
    "\n",
    "Variações das LSTMs, como as unidades recorrentes com portões (Gated Recurrent Networks, ou GRUs) enriqueceram ainda mais este tema e hoje representam mais uma abordagem principal para entender as RNNs.\n",
    "\n",
    "### Leituras adicionais\n",
    "\n",
    "Caso você queira se aprofundar mais em alguns dos assuntos abordados nessa seção, recomendamos a leitura dos seguintes links:\n",
    "\n",
    "* [Desaparecimento do gradiente](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "* [Série geométrica](https://socratic.org/algebra/exponents-and-exponential-functions/geometric-sequences-and-exponential-functions)\n",
    "* [TDNN](https://en.wikipedia.org/wiki/Time_delay_neural_network)\n",
    "* [Publicação original da Elman Network de 1990](onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/abstract)\n",
    "* [Informações adicionais das RNNs](https://en.wikipedia.org/wiki/Recurrent_neual_network#Elman_networks_and_Jordan_networks)\n",
    "* [LSTM](www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "* [Sepp Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter)\n",
    "* [Jürgen Schmidhuber](people.idsia.ch/~juergen/)\n",
    "* [GRU](https://deeplearning4j.org/lstm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aplicações\n",
    "\n",
    "As maiores empresas de tecnologia do mundo usam RNNs e LSTM em seus programas.\n",
    "\n",
    "Esses modelos podem ser usados no reconhecimento de fala, onde uma sequência de amostras de dados extraída de um sinal de áudio é continuamente convertida em texto. Bons exemplos são:\n",
    "\n",
    "* Google Assistant\n",
    "* Siri, da Apple\n",
    "* Alexa, da Amazon\n",
    "* Soluções Dragon, da Nuance\n",
    "\n",
    "Também podem ser usados para a pravisão de séries temporais:\n",
    "\n",
    "* Waze, onde padrões de trânsito em estradas específicas são previstas para ajudar os motoristas a otimizar seus trajetos\n",
    "* Netflix, onde são previstos qual o próximo filme que o cliente vai quere assistir\n",
    "* Fundos de hedge que prevêem a movimentação dos preços de ações com base em padrões históricos da mivimentação da Bolsa e outras condições do mercado que mudam com o tempo\n",
    "\n",
    "Podem ser usados também para processamento de linguagem natural (Natural Language Processing, ou NLP):\n",
    "\n",
    "* Tradução automática, usada pelo Google e pela Salesforce\n",
    "* Sistema de perguntas e respostas, como o Google Analytics\n",
    "* Chatbots usados pelo Google, Baidu e Slack\n",
    "\n",
    "O último exemplo de aplicação de RNNs que vamos citar nesta seção é o de reconhecimento de gestos, onde o modelo observa uma sequência de quadros de vídeo para determinar o gesto que o usuário fez. Empresas como Intel, Cognitec e EyeSight usam o reconhecimento de gestos em seus produtos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Redes Neurais Feedforward (revisão)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

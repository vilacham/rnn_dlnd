{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neurais recorrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução às RNNs\n",
    "\n",
    "As arquiteturas de redes neurais vistas até agora foram treinadas usando apenas entradas atuais. Nelas, não são consideradas entradas anteriores ao gerar a saída atual. Em outras palavras, nossos sistemas não continham nenhum elemento de **memória**. As RNNs lidam com essa questão básica e importante usando **memória** (ou seja, entradas passadas da rede) ao produzir a entrada atual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## História das RNNs\n",
    "\n",
    "Depois da primeira onda de redes neurais artificiais, nos anos 1980, ficou claro que as redes feedforward são limitadas, já que não são capazes de capturar dependências temporais. Modelar dependências temporais é crucial na maioria dos programas do mundo real, já que os sinais naturais, como a fala e o vídeo, têm propriedades de variação de tempo e se caracterizam por ter dependências ao longo do tempo. Além disso, as redes neurais biológicas também têm conexões recorrentes, então aplicar recorrência às redes neurais feedforward artificiais fez todo sentido.\n",
    "\n",
    "A primeira tentativa de adicionar memória às redes neurais ocorreu em 1989, através das redes neurais com atraso de tempo (Time Delay Neural Network, ou TDNN). Nelas, as entradas de timesteps passados são introduzidas na entrada da rede, alterando as verdadeiras entradas externas. Isso trouxe a clara vantagem de permitir que a rede visse além do timestep atual, mas também trouxe uma desvantagem inegável, já que as dependências temporais ficam restritas à janela de tempo escolhida.\n",
    "\n",
    "<img src='../img/tdnn.png' width=350px>\n",
    "\n",
    "As RNNs simples, conhecidas também como redes de Elman e redes de Jordan, surgiram em 1990. Falaremos mais sobre elas ao longo da aula.\n",
    "\n",
    "<img src='../img/rnn_elman.png' width='350px'>\n",
    "\n",
    "Foi constatado, no início dos anos 1990, que todas essas redes sofrem do chamado \"problema da dissipição do gradiente\" (the vanishing gradient problem), onde as contribuições de informação deterioram-se geometricamente ao longo do tempo e, por conta disso, capturar relações que envolvem mais de 8 ou 10 passos anteriores é praticamente impossível. Como você deve lembrar, ao treinarmos a rede usamos **retropropagação**, processo no qual as matrizes de peso são ajustadas usando um **gradiente**. Os gradientes são calculados pela multiplicação sucessiva de derivadas, e o valor dessas derivadas pode ser tão pequeno que essas multiplicações sucessivas fazem com que o gradiente praticamente desapareça. O problema da dissipação do gradiente será discutido mais a fundo em breve. Apesar da elegância das RNNs que existiam até então, todas tinham essa grande falha. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
